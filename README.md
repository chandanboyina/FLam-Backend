# QueueCTL: Python-based Job Queue Management System

QueueCTL is a lightweight, reliable **CLI-driven distributed job queue system** built in Python.  
It supports **asynchronous job execution**, **persistent storage**, **multiple worker management**, **retry & backoff**, **dead letter queues (DLQ)**, **configurable settings**, and an optional **web dashboard for live monitoring**.

---

## Features Overview

| Category | Description |
|-----------|--------------|
| **Core Functionality** | Job enqueueing, execution, state tracking |
| **Persistence** | SQLite-backed durable storage (survives restarts) |
| **Workers** | Multiple worker processes with job locking |
| **Retry & Backoff** | Automatic retry with exponential delay |
| **Dead Letter Queue** | Failed jobs moved to DLQ after `max_retries` |
| **Configuration Management** | CLI-based config editing (`max_retries`, backoff) |
| **Metrics** | Average duration, job counts per state |
| **Web Dashboard** | Minimal monitoring UI (Flask-based) |
| **Output Logging** | Captures job stdout/stderr for review |
| **Timeout Handling** | Jobs automatically killed after `timeout_sec` |
| **Priority Queues** | Jobs processed based on priority value |
| **Scheduled Jobs** | Jobs delayed until specific `run_at` timestamps |

---

## Architecture Overview

###  **System Components**

```

QUEUECTL/
â”‚
â”œâ”€â”€ .venv/                        # Python virtual environment (dependencies isolated)
â”œâ”€â”€ logs/                         # Application and worker log files
â”‚
â”œâ”€â”€ queuectl/
â”‚   â”œâ”€â”€ __pycache__/              # Compiled Python bytecode
â”‚   â”œâ”€â”€ templates/                # HTML templates for Flask dashboard
â”‚   â”œâ”€â”€ __init__.py               # Package initializer
â”‚   â”œâ”€â”€ cli.py                    # Command-line interface for job control
â”‚   â”œâ”€â”€ config.py                 # Configuration and environment management
â”‚   â”œâ”€â”€ dashboard.py              # Flask-based web dashboard for job monitoring
â”‚   â”œâ”€â”€ db.py                     # SQLite persistence layer for job storage
â”‚   â”œâ”€â”€ models.py                 # ORM-like model definitions for queue entities
â”‚   â”œâ”€â”€ utils.py                  # Utility helpers, logging, and time functions
â”‚   â”œâ”€â”€ worker.py                 # Core worker logic for job execution
â”‚   â””â”€â”€ queue.db                  # Auto-generated SQLite database file
â”‚
â”œâ”€â”€ queuectl.egg-info/            # Package metadata (auto-generated by setuptools)
â”‚
â”œâ”€â”€ tests/                        # Unit and smoke tests
â”‚   â””â”€â”€ smoke.sh                  # Basic smoke test script
â”‚
â”œâ”€â”€ venv/                         # Optional additional virtual environment
â”‚
â”œâ”€â”€ LICENSE                       # License information
â”œâ”€â”€ pyproject.toml                # Project configuration (build system, dependencies)
â”œâ”€â”€ README.md                     # Project documentation (this file)
â”œâ”€â”€ smoke.db                      # Database created during smoke testing
â””â”€â”€ queue.db                      # Root-level database instance (if used globally)

```


---

## **Job Lifecycle**

| **State** | **Description** |
|------------|-----------------|
| `pending` | Waiting to be picked up by a worker |
| `processing` | Currently being executed |
| `completed` | Successfully executed |
| `failed` | Failed, but retryable |
| `dead` | Permanently failed (moved to DLQ) |

---

## Setup Instructions

### Clone Repository
```bash
git clone https://github.com/chandanboyina/FLam-Backend.git
```

```bash
cd queuectl
```

## **Create Virtual Environment**

```bash
python -m venv venv
venv\Scripts\activate   # (Windows)
# or
source venv/bin/activate  # (Linux/Mac)
```

## **Install Dependencies**

```bash
pip install -e .
```

## **Initialize Database**

```bash
python -m queuectl.db
```
## **Usage Example**

-----

### Enqueue Jobs

Adds jobs to the Queue

```bash
queuectl enqueue "{\"id\":\"job1\",\"command\":\"cmd /c echo Hello from QueueCTL\"}"
queuectl enqueue "{\"id\":\"job2\",\"command\":\"cmd /c exit 1\"}"
queuectl enqueue "{\"id\":\"delayed\",\"command\":\"cmd /c echo Delayed Job\",\"run_at\":\"2025-11-08T10:00:00Z\"}"
queuectl enqueue "{\"id\":\"timeout_job\",\"command\":\"cmd /c timeout 70\",\"timeout_sec\":5}"
```

### Start Workers

```bash
queuectl worker start --count 2

```

Starts 2 workers that process jobs concurrently

### Check Status

```bash
queuectl status

```
 
It prints a table of job Lifecycle Status

#### Example Output
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ State            â”ƒ Count â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ pending          â”‚     1 â”‚
â”‚ processing       â”‚     0 â”‚
â”‚ completed        â”‚     3 â”‚
â”‚ failed           â”‚     0 â”‚
â”‚ dead             â”‚     1 â”‚
â”‚ dlq              â”‚     1 â”‚
â”‚ avg_duration_sec â”‚  0.04 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

```

### List Jobs by State

```bash
queuectl list --state pending

```

### DLQ Management

```bash
queuectl dlq list
queuectl dlq retry job2
```

### Configuration Commands

```bash
queuectl config show
queuectl config set max-retries 5
queuectl config set base_backoff 3
```

### Stop Workers Grecefully

```bash
queuectl worker stop

```

### Launch Dashboard 
```bash
python -m queuectl.dashboard
```

[!Dashboard](https://github.com/chandanboyina/FLam-Backend/blob/main/Queuectl%20Dashboard.jpg)


Open: https://localhost:8080

-----

##  Architecture Explanation

QueueCTL is designed with a **modular and extensible architecture** following the *Single Responsibility Principle (SRP)* â€” each module handles one clear purpose.  

### Core Components

| **Module** | **Role** | **Detailed Description** |
|-------------|-----------|---------------------------|
| `cli.py` | Command Line Interface | Handles all CLI commands (`enqueue`, `list`, `status`, `worker`, `dlq`, `config`). It uses **Typer/Click** to parse commands and send actions to the core modules. |
| `db.py` | Database Persistence Layer | Manages the SQLite database. Creates tables for `jobs`, `dlq`, and `config`. Ensures **data durability**, so jobs persist after restarts. Uses SQL transactions for safe updates. |
| `worker.py` | Worker Logic | Core engine of the system. Each worker picks jobs in `pending` state, locks them, executes their command, tracks exit codes, updates job states (`processing â†’ completed/failed`), applies **timeouts** and **retries**, and pushes to DLQ when necessary. |
| `scheduler.py` | Job Scheduling | Manages **delayed jobs** using `run_at` timestamps. Ensures jobs are not executed before their scheduled time. |
| `config.py` | Configuration Manager | Stores and retrieves global configurations like `max_retries` and `backoff_base`. Config values are stored persistently in the database and can be changed dynamically from the CLI. |
| `dashboard.py` | Monitoring Dashboard | A **Flask web server** providing a live visualization of queue metrics, job counts, DLQ contents, and success/failure stats. It serves `dashboard.html` with Chart.js for live charts. |
| `utils.py` | Helper Functions | Contains reusable utilities for logging, time formatting, exponential backoff calculation, and process management. Keeps code DRY and clean. |
| `queue.db` | SQLite Database | Central persistent storage of all jobs, DLQ entries, configuration, and logs. |

---

### Data Flow Overview

1. **CLI Command** â†’ (e.g., `enqueue`, `worker start`)
2. **Job Stored** â†’ `db.py` inserts into `jobs` table.
3. **Worker Fetches** â†’ `worker.py` locks and picks one `pending` job.
4. **Job Execution** â†’ Worker executes `command` using `subprocess`.
5. **State Transition** â†’ Updates jobâ€™s state in database.
6. **Retry/Backoff** â†’ If failure, retries with exponential delay.
7. **DLQ Transfer** â†’ After max retries, moves to DLQ.
8. **Dashboard Visualization** â†’ Flask dashboard reads DB and shows live metrics.

---

### Design Principles Used

- **SRP (Single Responsibility Principle)** â†’ Each module has one clear function.  
- **Persistence First** â†’ Job data stored before execution ensures fault tolerance.  
- **Locking Mechanism** â†’ Prevents multiple workers from processing the same job.  
- **Graceful Shutdown** â†’ Workers finish running jobs before termination.  
- **Scalability** â†’ Multiple workers can run in parallel.  
- **Observability** â†’ Real-time dashboard shows internal metrics.

**Result:**  
This architecture makes QueueCTL robust, maintainable, and easy to extend with new features like priority queues or remote execution.


## âš™ï¸ Key Algorithms

QueueCTL implements several key control algorithms that ensure job execution is **fault-tolerant**, **efficient**, and **predictable**.

---

### Exponential Backoff (Retry Delay)

**Purpose:**  
Prevents retry storms when a job fails repeatedly by adding increasing delays.

**Formula:**  
```bash
delay = base_backoff ^ attempts
```


**Example:**
If `base_backoff = 2` and `max_retries = 3`:
| Attempt | Delay (seconds) |
|----------|-----------------|
| 1 | 2 |
| 2 | 4 |
| 3 | 8 |

**Implementation:**  
- Stored in `worker.py`
- When a job fails, it calculates next `run_at` using this delay before retrying.
- On final failure, moves job to DLQ.

---

### â±ï¸ Job Timeout Handling

**Purpose:**  
Ensures no job runs indefinitely (e.g., a stuck command).

**Mechanism:**
- Each job has a `timeout_sec` field (default = 60s).
- `worker.py` uses `subprocess.Popen` with a timer.
- If process runs longer than `timeout_sec`, it is **force-terminated** and marked `failed`.

**Result:**  
Your queue remains responsive even if one job misbehaves.

---

###  Retry Logic

**Purpose:**  
Ensures transient failures are automatically retried.

**Mechanism:**
- On failure, increment `attempts` count.
- If `attempts < max_retries` â†’ retry job after exponential delay.
- If `attempts == max_retries` â†’ move job to DLQ.

**SQL Example:**
```sql
UPDATE jobs SET state='pending', run_at='NOW()+delay' WHERE id=?;
```

**Result:**
Jobs recover automatically from network issues or temporary failures.

## Worker Locking

**Purpose:**
Prevents two workers from picking the same job concurrently.

**Mechanism:**
* SQLite BEGIN IMMEDIATE TRANSACTION ensures row-level lock.
* Worker atomically updates job state to processing.
* Other workers see the job as unavailable.

**Result:**
No duplicate job executions even when multiple workers run in parallel.


---

## ğŸ§® Metrics Aggregation

**Purpose:**  
Continuously update and display real-time queue statistics for monitoring and analysis.

### Metrics Tracked
- **Job counts per state** â€” Pending, Running, Completed, Failed  
- **DLQ (Dead Letter Queue) count** â€” Tracks permanently failed jobs  
- **Average duration:** `AVG(duration_sec)` for completed jobs

### Used In
- **CLI:**  
  Displayed via the `status` command for quick summary.  
- **Dashboard:**  
  Exposed through the `/api/stats` endpoint for live charts and analytics.

---

## Delayed Job Scheduling

**Purpose:**  
Enable scheduling of jobs to execute at a **future timestamp** using `run_at`.

### Mechanism
- The **scheduler** filters jobs where:
  ```sql
  run_at <= NOW()






